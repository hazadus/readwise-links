"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–µ–π –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞ Readwise Reader.

- –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞ articles.json, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.

- –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS –∏ CSS, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –∏
—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

- –ó–∞—Ç–µ–º –∏–∑–º–µ–Ω—è–µ—Ç –≤ HTML —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π
HTML –≤ —Ñ–∞–π–ª index.html.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞):
    uv run ./app/scrape.py
"""

import asyncio
import json
import logging
import os
from asyncio import Lock, Queue, Semaphore
from pathlib import Path
from time import perf_counter
from urllib.parse import urljoin, urlparse
from uuid import uuid4

import aiofiles
import httpx
from bs4 import BeautifulSoup
from logger import setup_logging
from schemas.readwise import EnrichedReadwiseDocument

# –î–ª—è –ø—Ä–æ—Å—Ç—ã —Ö–∞—Ä–¥–∫–æ–¥–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–µ –≤—ã–¥–µ–ª—è—è –∏—Ö –≤ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥.
# –û–Ω–∏ –Ω–µ –±—É–¥—É—Ç –º–µ–Ω—è—Ç—å—Å—è, –∞ –µ—Å–ª–∏ –±—É–¥—É—Ç, —Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ.
ARCHIVE_DIR = "./scratch/archive"
MAX_CACHE_SIZE = 1000 * 1024 * 1024  # 1000MB
MAX_SCRAPE_WORKERS = 8  # –ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —è–¥–µ—Ä –≤ M1
MAX_DOWNLOAD_WORKERS = 16  # x2 –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —è–¥–µ—Ä
HTTPX_TIMEOUT = 10  # —Å–µ–∫—É–Ω–¥
STOP_TOKEN = object()  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç –∫–∞–∫ —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–∞–±–æ—á–∏–º

logger = logging.getLogger(__name__)
request_semaphore = Semaphore(20)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
file_semaphore = Semaphore(8)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª-–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
download_cache: dict[str, bytes] = {}  # URL -> content
current_cache_size = 0
cache_size_lock = Lock()


async def main():
    # –ó–∞–≥—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞ "./web/src/assets/articles.json"
    articles = load_articles_from_file(
        filepath=Path("./web/src/assets/articles.json"),
    )
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} –ø–æ—Å—Ç–æ–≤")

    # –ù–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–∞–Ω–µ–µ - –ø–æ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    # –∏ –∏–º–µ–µ—Ç id - –∏–Ω–∞—á–µ –Ω–µ —Å–º–æ–∂–µ–º –ø–æ—Ç–æ–º –ø–æ–Ω—è—Ç—å, —á—Ç–æ —ç—Ç–æ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    scrape_queue = Queue()
    [
        scrape_queue.put_nowait(doc)
        for doc in articles
        if doc.source_url is not None
        and doc.category == "article"
        and doc.id is not None
        and not os.path.exists(Path(ARCHIVE_DIR) / doc.id / "index.html")
    ]
    print(f"–í –æ—á–µ—Ä–µ–¥–∏ {scrape_queue.qsize()} —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    scrapers = []
    try:
        async with httpx.AsyncClient(
            timeout=HTTPX_TIMEOUT,
            follow_redirects=True,
        ) as client:
            scrapers = [
                asyncio.create_task(
                    scrape_worker(
                        worker_id=worker_id,
                        scrape_queue=scrape_queue,
                        output_dir=ARCHIVE_DIR,
                        client=client,
                    )
                )
                for worker_id in range(MAX_SCRAPE_WORKERS)
            ]

            # –î–æ–±–∞–≤–ª—è–µ–º STOP_TOKEN –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            # –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            for _ in range(MAX_SCRAPE_WORKERS):
                scrape_queue.put_nowait(STOP_TOKEN)

            start = perf_counter()
            await scrape_queue.join()
            elapsed_time = perf_counter() - start
            print(f"üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫.")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ Ctrl+C")
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
    finally:
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –∑–∞–¥–∞—á–∏ –æ—Ç–º–µ–Ω–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        for task in scrapers:
            if not task.done():
                task.cancel()

        if scrapers:
            # –°–æ–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –æ—Ç–º–µ–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
            await asyncio.gather(*scrapers, return_exceptions=True)


async def scrape_worker(
    *,
    worker_id: int,
    scrape_queue: Queue,
    output_dir: str,
    client: httpx.AsyncClient,
):
    """
    –†–∞–±–æ—á–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü. –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞
    –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS –∏ CSS, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏. –ó–∞—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π HTML –≤ —Ñ–∞–π–ª.

    :param worker_id: ID —Ä–∞–±–æ—á–µ–≥–æ
    :param scrape_queue: –û—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
    :param client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
    """
    while True:
        doc = await scrape_queue.get()
        try:
            if doc is STOP_TOKEN:
                break

            url = doc.source_url

            logger.info(
                f"Worker S-{worker_id} | –û—Å—Ç–∞–ª–æ—Å—å: {scrape_queue.qsize()} | üöÄ –°–∫–∞—á–∏–≤–∞—é {url}"
            )
            data = await download_url_cached(
                url=url,
                client=client,
            )
            if data is None:
                logger.error(f"Worker S-{worker_id} | ‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}")
                continue

            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è, —Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            html = None
            encodings_to_try = ["utf-8", "latin-1", "windows-1252", "iso-8859-1"]

            for encoding in encodings_to_try:
                try:
                    html = data.decode(encoding=encoding)
                    logger.debug(
                        f"Worker S-{worker_id} | –£—Å–ø–µ—à–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–ª {url} –∫–∞–∫ {encoding}"
                    )
                    break
                except UnicodeDecodeError:
                    continue

            if html is None:
                logger.error(
                    f"Worker S-{worker_id} | ‚ùå –ù–µ —Å–º–æ–≥ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {url} —Å –ø–æ–º–æ—â—å—é "
                    f"{', '.join(encodings_to_try)}"
                )
                continue

            # –í—ã—Ç–∞—â–∏—Ç—å –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS
            links_map = get_all_links_from_html(
                url=url,
                html=html,
            )

            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS –ø–æ —Å—Å—ã–ª–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤
            # —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            doc_output_dir = Path(output_dir) / doc.id
            all_links = list(links_map.values())
            names = await download_links(
                links=all_links,
                output_dir=doc_output_dir,
                client=client,
            )

            # –ó–∞–º–µ–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
            for link in links_map.keys():
                # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞, c–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞—è –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ –∏—Å—Ö–æ–¥–Ω—É—é (–∏–∑ HTML)
                absolute_link = links_map[link]
                filename = names.get(absolute_link)
                # –ó–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å–∫–∞—á–∞–Ω
                if filename is not None:
                    html = html.replace(link, filename)
                else:
                    # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å–∫–∞—á–∞–Ω, —Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º –≤–∏–¥–µ
                    html = html.replace(link, absolute_link)

            # C–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π HTML –≤ —Ñ–∞–π–ª
            filepath = Path(doc_output_dir) / "index.html"
            await save_to_file(
                filepath=filepath,
                content=html.encode(encoding="utf-8"),
            )
            logger.info(f"Worker S-{worker_id} | üì• –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")
        except Exception as e:
            logger.error(
                f"Worker S-{worker_id} | ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {doc.source_url}: {e}"
            )
        finally:
            # –í—Å–µ–≥–¥–∞ –æ—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            scrape_queue.task_done()


async def download_links(
    *,
    links: list[str],
    output_dir: str,
    client: httpx.AsyncClient,
) -> dict[str, str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å—Å—ã–ª–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

    :param links: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    :param client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
    :return: –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    download_queue = Queue()
    [download_queue.put_nowait(link) for link in links]

    # –î–æ–±–∞–≤–ª—è–µ–º STOP_TOKEN –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    # –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    for _ in range(MAX_DOWNLOAD_WORKERS):
        download_queue.put_nowait(STOP_TOKEN)

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    links_to_filenames = {}
    download_lock = Lock()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    try:
        downloaders = [
            asyncio.create_task(
                download_worker(
                    worker_id=worker_id,
                    download_queue=download_queue,
                    download_lock=download_lock,
                    output_dir=output_dir,
                    links_to_filenames=links_to_filenames,
                    client=client,
                )
            )
            for worker_id in range(MAX_DOWNLOAD_WORKERS)
        ]

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—é –æ—á–µ—Ä–µ–¥—å –≤–æ—Ä–∫–µ—Ä–∞–º–∏
        await download_queue.join()
        for task in downloaders:
            task.cancel()
        await asyncio.gather(*downloaders, return_exceptions=True)
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        for task in downloaders:
            if not task.done():
                task.cancel()

    return links_to_filenames


async def download_worker(
    *,
    worker_id: int,
    download_queue: Queue,
    download_lock: Lock,
    output_dir: str,
    links_to_filenames: dict[str, str],
    client: httpx.AsyncClient,
) -> None:
    """
    –†–∞–±–æ—á–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤. –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π
    –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å links_to_filenames —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É
    —Å—Å—ã–ª–∫–æ–π –∏ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞.

    :param worker_id: ID —Ä–∞–±–æ—á–µ–≥–æ
    :param download_queue: –û—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param download_lock: –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–ª–æ–≤–∞—Ä—é
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    :param links_to_filenames: –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤
    :param client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
    """
    while True:
        link = await download_queue.get()
        try:
            if link is STOP_TOKEN:
                break

            filename = create_filename(url=link)
            filepath = Path(output_dir) / filename

            logger.info(f"Worker D-{worker_id} | –°–∫–∞—á–∏–≤–∞—é {link} to {filepath}")
            data = await download_url_cached(
                url=link,
                client=client,
            )
            if data is None:
                logger.error(f"Worker D-{worker_id} | –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {link}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
                async with download_lock:
                    links_to_filenames[link] = link
                continue

            await save_to_file(
                filepath=filepath,
                content=data,
            )
            logger.debug(f"Worker D-{worker_id} | üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å—Å—ã–ª–∫–æ–π –∏ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
            async with download_lock:
                links_to_filenames[link] = filename

        except Exception as e:
            logger.error(f"Worker D-{worker_id} | ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")
            # –í —Å–ª—É—á–∞–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è, —Ç–æ–∂–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
            try:
                async with download_lock:
                    links_to_filenames[link] = link
            except Exception:
                pass
        finally:
            # –í—Å–µ–≥–¥–∞ –æ—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            download_queue.task_done()


def get_all_links_from_html(
    *,
    url: str,
    html: str,
) -> dict[str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS –∏ CSS –∏–∑ HTML-–∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    :param url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :param html: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :return: –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏—Ö –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ URL
    """
    soup = BeautifulSoup(html, "html.parser")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã —Å—Å—ã–ª–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä BeautifulSoup
    css_links = [
        link.get("href")
        for link in soup.find_all(name="link", attrs={"rel": "stylesheet"})
        if link.get("href")
    ]

    img_links = [
        img.get("src")
        for img in soup.find_all(name="img")
        if img.get("src") and not img.get("src").startswith("data:")
    ]

    js_links = [
        script.get("src")
        for script in soup.find_all(name="script")
        if script.get("src")
    ]

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ
    # –∫–ª—é—á - —Å—Å—ã–ª–∫–∞ –∏–∑ HTML, –∑–Ω–∞—á–µ–Ω–∏–µ - –∞–±—Å–æ–ª—é—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    links_map = {}
    for link in css_links + img_links + js_links:
        links_map[link] = (
            urljoin(url, link)
            if not (link.startswith("http") or link.startswith("//"))
            else link
        )
    return links_map


def create_filename(
    *,
    url: str,
) -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL. –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞—Å
    –Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç, –ø–æ—ç—Ç–æ–º—É –Ω–æ–≤–æ–µ –∏–º—è –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å–ª—É—á–∞–π–Ω–æ.

    :param url: URL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    :return: –ò–º—è —Ñ–∞–π–ª–∞
    """
    path = urlparse(url).path
    if "." in path:
        extension = path.split(".")[-1]
        return f"{uuid4()}.{extension}"
    return f"{uuid4()}"


async def download_url_cached(
    *,
    url: str,
    client: httpx.AsyncClient,
) -> bytes | None:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–µ—à–∞.
    –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –≤ –∞—Ä—Ö–∏–≤–∞ –º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Å–∞–π—Ç–æ–≤,
    –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ—Å—É—Ä—Å—ã.

    :param url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :return: –ö–æ–Ω—Ç–µ–Ω—Ç –≤ –±–∞–π—Ç–∞—Ö –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    global current_cache_size

    if url in download_cache.keys():
        logger.info(
            f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à –¥–ª—è {url} (—Ä–∞–∑–º–µ—Ä –∫—ç—à–∞: {current_cache_size} –±–∞–π—Ç)"
        )
        return download_cache[url]

    result = await download_url(
        url=url,
        client=client,
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—Å—è –ª–∏ –∫–µ—à –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    async with cache_size_lock:
        size = len(result) if result is not None else 0
        if current_cache_size + size <= MAX_CACHE_SIZE:
            logger.debug(f"üîÑ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à –¥–ª—è {url}")
            download_cache[url] = result
            current_cache_size += size
        else:
            logger.debug("‚ùå –ö–µ—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, –±–æ–ª—å—à–µ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º")

    return result


async def download_url(
    *,
    url: str,
    client: httpx.AsyncClient,
) -> bytes | None:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL.

    :param url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param client: HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :return: –ö–æ–Ω—Ç–µ–Ω—Ç –≤ –±–∞–π—Ç–∞—Ö –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    async with request_semaphore:
        try:
            response = await client.get(
                url=url,
                timeout=HTTPX_TIMEOUT,
                # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥—É–µ–º –∑–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏
                follow_redirects=True,
            )
        except httpx.ConnectTimeout as e:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
            return None
        except httpx.RequestError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
            return None

        if response.status_code == 200:
            return response.content
        else:
            logger.warning(f"‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}: {response.status_code}")
            return None


async def save_to_file(
    *,
    filepath: Path,
    content: bytes,
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª.

    :param filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    :param content: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    filepath.parent.mkdir(parents=True, exist_ok=True)
    async with file_semaphore:
        async with aiofiles.open(filepath, "wb") as f:
            await f.write(content)


def load_articles_from_file(
    *,
    filepath: Path,
) -> list[EnrichedReadwiseDocument]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞ articles.json.

    :param filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É articles.json
    :return: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
    """
    if not filepath.exists():
        logger.error(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return []

    with open(filepath, "r", encoding="utf-8") as f:
        data = f.read()

    parsed_data = json.loads(data)
    articles = [EnrichedReadwiseDocument.model_validate(doc) for doc in parsed_data]
    return articles


if __name__ == "__main__":
    setup_logging()
    asyncio.run(main())
