import asyncio
import json
import logging
import os
from asyncio import Lock, Queue
from pathlib import Path
from time import perf_counter
from urllib.parse import urljoin, urlparse
from uuid import uuid4

import aiofiles
import httpx
from bs4 import BeautifulSoup
from logger import setup_logging
from schemas.readwise import EnrichedReadwiseDocument

logger = logging.getLogger(__name__)

ARCHIVE_DIR = "./scratch/archive"
MAX_DOWNLOAD_WORKERS = 5
MAX_SCRAPE_WORKERS = 5
HTTPX_TIMEOUT = 10  # —Å–µ–∫—É–Ω–¥


async def main():
    # –ó–∞–≥—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞ "./web/src/assets/articles.json"
    articles = load_articles_from_file(
        filepath=Path("./web/src/assets/articles.json"),
    )
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} –ø–æ—Å—Ç–æ–≤")

    # –°—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

    # –ù–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–∞–Ω–µ–µ - –ø–æ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    # –∏ –∏–º–µ–µ—Ç id - –∏–Ω–∞—á–µ –Ω–µ —Å–º–æ–∂–µ–º –ø–æ—Ç–æ–º –ø–æ–Ω—è—Ç—å, —á—Ç–æ —ç—Ç–æ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    scrape_queue = Queue()
    [
        scrape_queue.put_nowait(doc)
        for doc in articles
        if doc.source_url is not None
        and doc.category == "article"
        and doc.id is not None
        and not os.path.exists(Path(ARCHIVE_DIR) / doc.id)
    ]
    print(f"–í –æ—á–µ—Ä–µ–¥–∏ {scrape_queue.qsize()} —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    scrapers = [
        asyncio.create_task(
            scrape_worker(
                worker_id=worker_id,
                scrape_queue=scrape_queue,
                output_dir=ARCHIVE_DIR,
            )
        )
        for worker_id in range(MAX_SCRAPE_WORKERS)
    ]

    start = perf_counter()
    await asyncio.gather(
        scrape_queue.join(),
        *scrapers,
    )
    end = start - perf_counter()
    print(f"üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {end:.2f} —Å–µ–∫.")


async def scrape_worker(
    *,
    worker_id: int,
    scrape_queue: Queue,
    output_dir: str,
):
    while not scrape_queue.empty():
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        doc: EnrichedReadwiseDocument = scrape_queue.get_nowait()
        url = doc.source_url

        logger.info(f"Worker S-{worker_id} | –°–∫–∞—á–∏–≤–∞—é {url}")
        data = await download_url(url=url)
        if data is None:
            logger.error(f"Worker S-{worker_id} | ‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}")
            # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
            scrape_queue.task_done()
            continue

        html = data.decode(encoding="utf-8")

        # –í—ã—Ç–∞—â–∏—Ç—å –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS
        links_map = get_all_links_from_html(
            url=url,
            html=html,
        )

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS –ø–æ —Å—Å—ã–ª–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤
        # —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        doc_output_dir = Path(output_dir) / doc.id
        all_links = list(links_map.values())
        names = await download_links(
            links=all_links,
            output_dir=doc_output_dir,
        )

        # –ó–∞–º–µ–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
        for link in links_map.keys():
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞, c–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞—è –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ –∏—Å—Ö–æ–¥–Ω—É—é (–∏–∑ HTML)
            absolute_link = links_map[link]
            filename = names.get(absolute_link)
            # –ó–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
            html = html.replace(link, filename)

        # C–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π HTML –≤ —Ñ–∞–π–ª
        filepath = Path(doc_output_dir) / "index.html"
        await save_to_file(
            filepath=filepath,
            content=html.encode(encoding="utf-8"),
        )
        logger.info(f"Worker S-{worker_id} | üì• –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")

        # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
        scrape_queue.task_done()


def get_all_links_from_html(
    *,
    url: str,
    html: str,
) -> dict[str, str]:
    css_links = get_rel_links_from_html(
        html=html,
        rel_type="stylesheet",
    )

    img_links = get_img_links_from_html(
        html=html,
    )

    js_links = get_js_links_from_html(
        html=html,
    )

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    # –°—Å—ã–ª–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ : –ø–æ–ª–Ω–∞—è —Å—Å—ã–ª–∫–∞
    links_map = {}
    for link in css_links + img_links + js_links:
        links_map[link] = urljoin(url, link) if not link.startswith("http") else link
    return links_map


async def download_links(
    *,
    links: list[str],
    output_dir: str,
) -> dict[str, str]:
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    download_queue = Queue()
    [download_queue.put_nowait(link) for link in links]

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    links_to_filenames = {}
    download_lock = Lock()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    downloaders = [
        asyncio.create_task(
            download_worker(
                worker_id=worker_id,
                download_queue=download_queue,
                download_lock=download_lock,
                output_dir=output_dir,
                links_to_filenames=links_to_filenames,
            )
        )
        for worker_id in range(MAX_DOWNLOAD_WORKERS)
    ]

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—é –æ—á–µ—Ä–µ–¥—å –≤–æ—Ä–∫–µ—Ä–∞–º–∏
    await asyncio.gather(
        download_queue.join(),
        *downloaders,
    )

    return links_to_filenames


async def download_worker(
    *,
    worker_id: int,
    download_queue: Queue,
    download_lock: Lock,
    output_dir: str,
    links_to_filenames: dict[str, str],
) -> None:
    while not download_queue.empty():
        link = download_queue.get_nowait()
        filename = create_filename(url=link)
        filepath = Path(output_dir) / filename

        logger.info(f"Worker D-{worker_id} | –°–∫–∞—á–∏–≤–∞—é {link} to {filepath}")
        data = await download_url(url=link)
        if data is None:
            logger.error(f"Worker D-{worker_id} | –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {link}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
            async with download_lock:
                links_to_filenames[link] = link
            # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
            download_queue.task_done()
            continue

        await save_to_file(
            filepath=filepath,
            content=data,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å—Å—ã–ª–∫–æ–π –∏ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
        async with download_lock:
            links_to_filenames[link] = filename
        logger.info(f"Worker D-{worker_id} | –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")

        # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
        download_queue.task_done()


def get_rel_links_from_html(
    *,
    html: str,
    rel_type: str = "stylesheet",
) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for link in soup.find_all(name="link", attrs={"rel": rel_type}):
        href = link.get("href")
        if href:
            links.append(href)
    return links


def get_img_links_from_html(
    *,
    html: str,
) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for img in soup.find_all(name="img"):
        src = img.get("src")
        if src:
            links.append(src)
    return links


def get_js_links_from_html(
    *,
    html: str,
) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for script in soup.find_all(name="script"):
        src = script.get("src")
        if src:
            links.append(src)
    return links


def create_filename(
    *,
    url: str,
) -> str:
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL (—É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ —è–∫–æ—Ä—è)
    path = urlparse(url).path
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ path
    if not "." in path:
        # –ï—Å–ª–∏ –≤ URL –Ω–µ—Ç —Ç–æ—á–∫–∏, —Ç–æ –Ω–µ –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        extension = ""
    else:
        extension = path.split(".")[-1]

    if not extension:
        extension = ""
    return f"{uuid4()}.{extension}" if extension != "" else f"{uuid4()}"


async def download_url(
    *,
    url: str,
) -> bytes | None:
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(
                url=url,
                timeout=HTTPX_TIMEOUT,
                # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥—É–µ–º –∑–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏
                follow_redirects=True,
            )
        except httpx.ConnectTimeout as e:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
            return None
        except httpx.RequestError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
            return None

        if response.status_code == 200:
            return response.content
        else:
            logger.warning(f"‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}: {response.status_code}")
            return None


async def save_to_file(
    *,
    filepath: Path,
    content: bytes,
) -> None:
    filepath.parent.mkdir(parents=True, exist_ok=True)
    async with aiofiles.open(filepath, "wb") as f:
        await f.write(content)


def load_articles_from_file(
    *,
    filepath: Path,
) -> list[EnrichedReadwiseDocument]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞ articles.json.
    """
    if not filepath.exists():
        logger.error(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return []

    with open(filepath, "r", encoding="utf-8") as f:
        data = f.read()

    parsed_data = json.loads(data)
    articles = [EnrichedReadwiseDocument.model_validate(doc) for doc in parsed_data]
    return articles


if __name__ == "__main__":
    setup_logging()
    asyncio.run(main())
