"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–µ–π –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–∞ Readwise Reader.

- –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞ articles.json, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.

- –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS –∏ CSS, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –∏
—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

- –ó–∞—Ç–µ–º –∏–∑–º–µ–Ω—è–µ—Ç –≤ HTML —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π
HTML –≤ —Ñ–∞–π–ª index.html.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.
"""

import asyncio
import json
import logging
import os
from asyncio import Lock, Queue
from pathlib import Path
from time import perf_counter
from urllib.parse import urljoin, urlparse
from uuid import uuid4

import aiofiles
import httpx
from bs4 import BeautifulSoup
from logger import setup_logging
from schemas.readwise import EnrichedReadwiseDocument

logger = logging.getLogger(__name__)

ARCHIVE_DIR = "./scratch/archive"
MAX_DOWNLOAD_WORKERS = 5
MAX_SCRAPE_WORKERS = 5
HTTPX_TIMEOUT = 10  # —Å–µ–∫—É–Ω–¥


async def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞
    articles.json, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç
    –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.
    """
    # –ó–∞–≥—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞ "./web/src/assets/articles.json"
    articles = load_articles_from_file(
        filepath=Path("./web/src/assets/articles.json"),
    )
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} –ø–æ—Å—Ç–æ–≤")

    # –°—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏

    # –ù–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏
    # –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–∞–Ω–µ–µ - –ø–æ –∏–º–µ–Ω–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    # –∏ –∏–º–µ–µ—Ç id - –∏–Ω–∞—á–µ –Ω–µ —Å–º–æ–∂–µ–º –ø–æ—Ç–æ–º –ø–æ–Ω—è—Ç—å, —á—Ç–æ —ç—Ç–æ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    scrape_queue = Queue()
    [
        scrape_queue.put_nowait(doc)
        for doc in articles
        if doc.source_url is not None
        and doc.category == "article"
        and doc.id is not None
        and not os.path.exists(Path(ARCHIVE_DIR) / doc.id)
    ]
    print(f"–í –æ—á–µ—Ä–µ–¥–∏ {scrape_queue.qsize()} —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    scrapers = [
        asyncio.create_task(
            scrape_worker(
                worker_id=worker_id,
                scrape_queue=scrape_queue,
                output_dir=ARCHIVE_DIR,
            )
        )
        for worker_id in range(MAX_SCRAPE_WORKERS)
    ]

    start = perf_counter()
    await asyncio.gather(
        scrape_queue.join(),
        *scrapers,
    )
    elapsed_time = perf_counter() - start
    print(f"üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f} —Å–µ–∫.")


async def scrape_worker(
    *,
    worker_id: int,
    scrape_queue: Queue,
    output_dir: str,
):
    """
    –†–∞–±–æ—á–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü. –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞
    –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS –∏ CSS, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏. –ó–∞—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π HTML –≤ —Ñ–∞–π–ª.

    :param worker_id: ID —Ä–∞–±–æ—á–µ–≥–æ
    :param scrape_queue: –û—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
    """
    # –í –æ—á–µ—Ä–µ–¥—å –Ω–∏—á–µ–≥–æ –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º while
    # –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—á–µ—Ä–µ–¥—å –Ω–µ –ø—É—Å—Ç–∞
    while not scrape_queue.empty():
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        doc: EnrichedReadwiseDocument = scrape_queue.get_nowait()
        url = doc.source_url

        logger.info(f"Worker S-{worker_id} | –°–∫–∞—á–∏–≤–∞—é {url}")
        data = await download_url(url=url)
        if data is None:
            logger.error(f"Worker S-{worker_id} | ‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}")
            # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
            scrape_queue.task_done()
            continue

        # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è, —Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        html = None
        encodings_to_try = ["utf-8", "latin-1", "windows-1252", "iso-8859-1"]

        for encoding in encodings_to_try:
            try:
                html = data.decode(encoding=encoding)
                logger.debug(
                    f"Worker S-{worker_id} | –£—Å–ø–µ—à–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–ª {url} –∫–∞–∫ {encoding}"
                )
                break
            except UnicodeDecodeError:
                continue

        if html is None:
            logger.error(
                f"Worker S-{worker_id} | ‚ùå –ù–µ —Å–º–æ–≥ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {url} —Å –ø–æ–º–æ—â—å—é "
                f"{', '.join(encodings_to_try)}"
            )
            scrape_queue.task_done()
            continue

        # –í—ã—Ç–∞—â–∏—Ç—å –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS
        links_map = get_all_links_from_html(
            url=url,
            html=html,
        )

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, JS, CSS –ø–æ —Å—Å—ã–ª–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª—ã –≤ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤
        # —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        doc_output_dir = Path(output_dir) / doc.id
        all_links = list(links_map.values())
        names = await download_links(
            links=all_links,
            output_dir=doc_output_dir,
        )

        # –ó–∞–º–µ–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ
        for link in links_map.keys():
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞, c–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞—è –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ –∏—Å—Ö–æ–¥–Ω—É—é (–∏–∑ HTML)
            absolute_link = links_map[link]
            filename = names.get(absolute_link)
            # –ó–∞–º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ HTML –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
            html = html.replace(link, filename)

        # C–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–π HTML –≤ —Ñ–∞–π–ª
        filepath = Path(doc_output_dir) / "index.html"
        await save_to_file(
            filepath=filepath,
            content=html.encode(encoding="utf-8"),
        )
        logger.info(f"Worker S-{worker_id} | üì• –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")

        # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
        scrape_queue.task_done()


async def download_links(
    *,
    links: list[str],
    output_dir: str,
) -> dict[str, str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å—Å—ã–ª–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

    :param links: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    :return: –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    """
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    download_queue = Queue()
    [download_queue.put_nowait(link) for link in links]

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    links_to_filenames = {}
    download_lock = Lock()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.gather –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    downloaders = [
        asyncio.create_task(
            download_worker(
                worker_id=worker_id,
                download_queue=download_queue,
                download_lock=download_lock,
                output_dir=output_dir,
                links_to_filenames=links_to_filenames,
            )
        )
        for worker_id in range(MAX_DOWNLOAD_WORKERS)
    ]

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å—é –æ—á–µ—Ä–µ–¥—å –≤–æ—Ä–∫–µ—Ä–∞–º–∏
    await asyncio.gather(
        download_queue.join(),
        *downloaders,
    )

    return links_to_filenames


async def download_worker(
    *,
    worker_id: int,
    download_queue: Queue,
    download_lock: Lock,
    output_dir: str,
    links_to_filenames: dict[str, str],
) -> None:
    """
    –†–∞–±–æ—á–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤. –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π
    –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å links_to_filenames —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É
    —Å—Å—ã–ª–∫–æ–π –∏ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞.

    :param worker_id: ID —Ä–∞–±–æ—á–µ–≥–æ
    :param download_queue: –û—á–µ—Ä–µ–¥—å —Å—Å—ã–ª–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :param download_lock: –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–ª–æ–≤–∞—Ä—é
    :param output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    :param links_to_filenames: –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤
    """
    # –í –æ—á–µ—Ä–µ–¥—å –Ω–∏—á–µ–≥–æ –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º while
    # –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—á–µ—Ä–µ–¥—å –Ω–µ –ø—É—Å—Ç–∞
    while not download_queue.empty():
        link = download_queue.get_nowait()
        filename = create_filename(url=link)
        filepath = Path(output_dir) / filename

        logger.info(f"Worker D-{worker_id} | –°–∫–∞—á–∏–≤–∞—é {link} to {filepath}")
        data = await download_url(url=link)
        if data is None:
            logger.error(f"Worker D-{worker_id} | –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {link}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
            async with download_lock:
                links_to_filenames[link] = link
            # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
            download_queue.task_done()
            continue

        await save_to_file(
            filepath=filepath,
            content=data,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å—Å—ã–ª–∫–æ–π –∏ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
        async with download_lock:
            links_to_filenames[link] = filename
        logger.info(f"Worker D-{worker_id} | –°–æ—Ö—Ä–∞–Ω—ë–Ω {filepath}")

        # –û—Ç–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
        download_queue.task_done()


def get_all_links_from_html(
    *,
    url: str,
    html: str,
) -> dict[str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS, JS –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ HTML.

    :param url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :param html: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :return: –°–ª–æ–≤–∞—Ä—å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏ –∏ –ø–æ–ª–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
    """
    css_links = get_rel_links_from_html(
        html=html,
        rel_type="stylesheet",
    )

    img_links = get_img_links_from_html(
        html=html,
    )

    js_links = get_js_links_from_html(
        html=html,
    )

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    # –°—Å—ã–ª–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞ : –ø–æ–ª–Ω–∞—è —Å—Å—ã–ª–∫–∞
    links_map = {}
    for link in css_links + img_links + js_links:
        links_map[link] = (
            urljoin(url, link)
            if not (link.startswith("http") or link.startswith("//"))
            else link
        )
    return links_map


def get_rel_links_from_html(
    *,
    html: str,
    rel_type: str = "stylesheet",
) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS —Ñ–∞–π–ª—ã –∏–∑ HTML.

    :param html: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :param rel_type: –¢–∏–ø —Å—Å—ã–ª–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "stylesheet")
    :return: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ CSS —Ñ–∞–π–ª—ã. –°—Å—ã–ª–∫–∏ –∫–∞–∫ –µ—Å—Ç—å –≤ –∫–æ–¥–µ, –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """

    soup = BeautifulSoup(html, "html.parser")
    links = []
    for link in soup.find_all(name="link", attrs={"rel": rel_type}):
        href = link.get("href")
        if href:
            links.append(href)
    return links


def get_img_links_from_html(
    *,
    html: str,
) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ HTML.

    :param html: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :return: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –°—Å—ã–ª–∫–∏ –∫–∞–∫ –µ—Å—Ç—å –≤ –∫–æ–¥–µ, –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for img in soup.find_all(name="img"):
        src = img.get("src")
        if src:
            if not src.startswith("data:"):
                links.append(src)
    return links


def get_js_links_from_html(
    *,
    html: str,
) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ JS —Ñ–∞–π–ª—ã –∏–∑ HTML.

    :param html: HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    :return: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ JS —Ñ–∞–π–ª—ã. –°—Å—ã–ª–∫–∏ –∫–∞–∫ –µ—Å—Ç—å –≤ –∫–æ–¥–µ, –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for script in soup.find_all(name="script"):
        src = script.get("src")
        if src:
            links.append(src)
    return links


def create_filename(
    *,
    url: str,
) -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL.

    :param url: URL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    :return: –ò–º—è —Ñ–∞–π–ª–∞
    """
    path = urlparse(url).path
    if "." in path:
        extension = path.split(".")[-1]
        return f"{uuid4()}.{extension}"
    return f"{uuid4()}"


async def download_url(
    *,
    url: str,
) -> bytes | None:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É URL.

    :param url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    :return: –ö–æ–Ω—Ç–µ–Ω—Ç –≤ –±–∞–π—Ç–∞—Ö –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(
                url=url,
                timeout=HTTPX_TIMEOUT,
                # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–ª–µ–¥—É–µ–º –∑–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º–∏
                follow_redirects=True,
            )
        except httpx.ConnectTimeout as e:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
            return None
        except httpx.RequestError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
            return None

        if response.status_code == 200:
            return response.content
        else:
            logger.warning(f"‚ùå –ù–µ —Å–º–æ–≥ —Å–∫–∞—á–∞—Ç—å {url}: {response.status_code}")
            return None


async def save_to_file(
    *,
    filepath: Path,
    content: bytes,
) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∞–π–ª.

    :param filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    :param content: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    filepath.parent.mkdir(parents=True, exist_ok=True)
    async with aiofiles.open(filepath, "wb") as f:
        await f.write(content)


def load_articles_from_file(
    *,
    filepath: Path,
) -> list[EnrichedReadwiseDocument]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ñ–∞–π–ª–∞ articles.json.

    :param filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É articles.json
    :return: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
    """
    if not filepath.exists():
        logger.error(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return []

    with open(filepath, "r", encoding="utf-8") as f:
        data = f.read()

    parsed_data = json.loads(data)
    articles = [EnrichedReadwiseDocument.model_validate(doc) for doc in parsed_data]
    return articles


if __name__ == "__main__":
    setup_logging()
    asyncio.run(main())
